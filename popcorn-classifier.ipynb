{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for Kaggle popcorn competition\n",
    "\n",
    "Uses logistic regression to try and predict sentiment from a bag-of-words summary of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "# Expit is just the sigmoid function, written in C\n",
    "from scipy.special import expit\n",
    "\n",
    "data = pd.read_csv('even_better_features.csv', dtype='float32')\n",
    "\n",
    "ys = data.iloc[:, 0]\n",
    "data = data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['ones_column'] = np.ones_like(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from timeit import default_timer\n",
    "\n",
    "def timed(f):\n",
    "    \"\"\"Decorator to measure the runtime of a function and print it to\n",
    "    stdout. May not be required.\"\"\"\n",
    "    if hasattr(f, 'func_name'):\n",
    "        name = f.func_name\n",
    "    elif hasattr(f, 'im_func'):\n",
    "        name = f.im_func.func_name\n",
    "    elif hasattr(f, '__name__'):\n",
    "        name = f.__name__\n",
    "    else:\n",
    "        name = '<unknown>'\n",
    "\n",
    "    @wraps(f)\n",
    "    def inner(*args, **kwargs):\n",
    "        print(\"Calling {}\".format(name))\n",
    "        start = default_timer()\n",
    "        rv = f(*args, **kwargs)\n",
    "        taken = default_timer() - start\n",
    "        print(\"Call to {} took {}s\".format(name, taken))\n",
    "        return rv\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_sigmoid(x):\n",
    "    \"\"\"Computes sigmoid function, but in the numerically stable way used by Sklearn\"\"\"\n",
    "    gt_zero = x > 0\n",
    "    gt_zero_x = x[gt_zero]\n",
    "    not_gt_zero_x = x[~gt_zero]\n",
    "    rv = np.zeros_like(x)\n",
    "    rv[gt_zero] = 1.0 / (1 + np.exp(-gt_zero_x))\n",
    "    rv[~gt_zero] = np.exp(not_gt_zero_x) / (np.exp(not_gt_zero_x) + 1)\n",
    "    return rv\n",
    "\n",
    "def safe_log_sigmoid(x):\n",
    "    \"\"\"Compute log(sigmoid(x)) safely\"\"\"\n",
    "    rv = np.zeros(len(x))\n",
    "    mask = x > 0\n",
    "    rv[mask] = -np.log(1 + np.exp(-x[mask]))\n",
    "    rv[~mask] = x[~mask] - np.log(1 + np.exp(x[~mask]))\n",
    "    return rv\n",
    "\n",
    "def safe_log_one_minus_sigmoid(x):\n",
    "    \"\"\"Compute log(1 - sigmoid(x)) safely\"\"\"\n",
    "    rv = np.zeros(len(x))\n",
    "    mask = x > 0\n",
    "    rv[mask] = -x[mask] - np.log(1 + np.exp(-x[mask]))\n",
    "    rv[~mask] = -np.log(np.exp(x[~mask]) + 1)\n",
    "    return rv\n",
    "\n",
    "def logistic_gradient(weights, features, labels):\n",
    "    \"\"\"Compute the gradient of the cross-entropy error for logistic regression\"\"\"\n",
    "    predictions = expit(np.dot(features, weights))\n",
    "    error = predictions - labels\n",
    "    # We divide by label.size to try and keep numerical error under control\n",
    "    rv = np.dot(error, features) / labels.size\n",
    "    return rv\n",
    "\n",
    "def cross_entropy_error(weights, features, labels):\n",
    "    \"\"\"Cross-entropy error for logistic regression\"\"\"\n",
    "    dots = np.dot(features, weights)\n",
    "    vect = labels*safe_log_sigmoid(dots) + (1-labels)*safe_log_one_minus_sigmoid(dots)\n",
    "    rv = -np.mean(vect)\n",
    "    return rv\n",
    "\n",
    "def logistic_predict(weights, features):\n",
    "    \"\"\"Predict labels for the given features using the given weights\"\"\"\n",
    "    probs = expit(np.dot(features, weights))\n",
    "    rv = np.zeros(len(features))\n",
    "    rv[probs > 0.5] = 1\n",
    "    return rv\n",
    "\n",
    "def train_logistic_regression(labels, features):\n",
    "    \"\"\"Find a weight vector for logistic regression using the supplied\n",
    "    labels and features.\"\"\"\n",
    "    np_labels = np.array(labels)\n",
    "    \n",
    "    initial_w = 0.1 * np.random.random(features.shape[1])\n",
    "    assert initial_w.ndim == 1\n",
    "    \n",
    "    result = opt.fmin_cg(\n",
    "        cross_entropy_error, \n",
    "        initial_w, \n",
    "        fprime=logistic_gradient,\n",
    "        args=(features, np_labels),\n",
    "        maxiter=20\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1919566773\n",
      "[ 0.0038      0.0012      0.0026     ...,  0.0186      0.0026      0.49656694]\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy_error(np.random.random(data.shape[1]), data, ys))\n",
    "print(logistic_gradient(np.random.random(data.shape[1]), data, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.025558\n",
      "         Iterations: 10\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = train_logistic_regression(ys, data)\n",
    "predictions = logistic_predict(ws, data)\n",
    "# MAXIMUM OVERFIT\n",
    "np.sum(predictions != ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier using scikit-learn\n",
    "\n",
    "The following classifier uses scikit-learn's SVM implementation to predict whether the movie gets a thumbs up or thumbs down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct/total: 2322/2500\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# From example\n",
    "clf = svm.SVC()\n",
    "clf.fit(data[:2500], ys[:2500])\n",
    "predictions = clf.predict(data[2500:])\n",
    "print(\"Correct/total: {}/{}\".format(\n",
    "    np.sum(predictions == ys[2500:]), len(ys[:2500])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
