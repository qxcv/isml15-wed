{
 "metadata": {
  "name": "",
  "signature": "sha256:001b15d48ba61bf9a231c37bbe4fe0ed2551f50faaa228aad27f324ee66c03d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Classifier for Kaggle popcorn competition\n",
      "\n",
      "Uses logistic regression to try and predict sentiment from a bag-of-words summary of each review."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy.optimize as opt\n",
      "# Expit is just the sigmoid function, written in C\n",
      "from scipy.special import expit\n",
      "\n",
      "data = pd.read_csv('even_better_features.csv', dtype='float32')\n",
      "\n",
      "ys = data.iloc[:, 0]\n",
      "data = data.iloc[:, 1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data['ones_column'] = np.ones_like(ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from functools import wraps\n",
      "from timeit import default_timer\n",
      "\n",
      "def timed(f):\n",
      "    \"\"\"Decorator to measure the runtime of a function and print it to\n",
      "    stdout.\"\"\"\n",
      "    if hasattr(f, 'func_name'):\n",
      "        name = f.func_name\n",
      "    elif hasattr(f, 'im_func'):\n",
      "        name = f.im_func.func_name\n",
      "    elif hasattr(f, '__name__'):\n",
      "        name = f.__name__\n",
      "    else:\n",
      "        name = '<unknown>'\n",
      "\n",
      "    @wraps(f)\n",
      "    def inner(*args, **kwargs):\n",
      "        start = default_timer()\n",
      "        rv = f(*args, **kwargs)\n",
      "        taken = default_timer() - start\n",
      "        print(\"Call to {} took {}s\".format(name, taken))\n",
      "        return rv\n",
      "\n",
      "    return inner"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def safe_sigmoid(x):\n",
      "    \"\"\"Computes sigmoid function, but in the numerically stable way used by Sklearn\"\"\"\n",
      "    gt_zero = x > 0\n",
      "    gt_zero_x = x[gt_zero]\n",
      "    not_gt_zero_x = x[~gt_zero]\n",
      "    rv = np.zeros_like(x)\n",
      "    rv[gt_zero] = 1.0 / (1 + np.exp(-gt_zero_x))\n",
      "    rv[~gt_zero] = np.exp(not_gt_zero_x) / (np.exp(not_gt_zero_x) + 1)\n",
      "    return rv\n",
      "\n",
      "def safe_log_sigmoid(x):\n",
      "    \"\"\"Compute log(sigmoid(x)) safely\"\"\"\n",
      "    rv = np.zeros(len(x))\n",
      "    mask = x > 0\n",
      "    rv[mask] = -np.log(1 + np.exp(-x[mask]))\n",
      "    rv[~mask] = x[~mask] - np.log(1 + np.exp(x[~mask]))\n",
      "    return rv\n",
      "\n",
      "def safe_log_one_minus_sigmoid(x):\n",
      "    \"\"\"Compute log(1 - sigmoid(x)) safely\"\"\"\n",
      "    rv = np.zeros(len(x))\n",
      "    mask = x > 0\n",
      "    rv[mask] = -x[mask] - np.log(1 + np.exp(-x[mask]))\n",
      "    rv[~mask] = -np.log(np.exp(x[~mask]) + 1)\n",
      "    return rv\n",
      "\n",
      "@timed\n",
      "def logistic_gradient(weights, features, labels):\n",
      "    \"\"\"Compute the gradient of the cross-entropy error for logistic regression\"\"\"\n",
      "    predictions = expit(np.dot(features, weights))\n",
      "    error = predictions - labels\n",
      "    # We divide by label.size to try and keep numerical error under control\n",
      "    return np.dot(error, features) / labels.size\n",
      "\n",
      "@timed\n",
      "def cross_entropy_error(weights, features, labels):\n",
      "    \"\"\"Cross-entropy error for logistic regression\"\"\"\n",
      "    dots = np.dot(features, weights)\n",
      "    rv = labels*safe_log_sigmoid(dots) + (1-labels)*safe_log_one_minus_sigmoid(dots)\n",
      "    return -np.mean(rv)\n",
      "\n",
      "def logistic_predict(weights, features):\n",
      "    \"\"\"Predict labels for the given features using the given weights\"\"\"\n",
      "    probs = expit(np.dot(features, weights))\n",
      "    rv = np.zeros(len(features))\n",
      "    rv[probs > 0.5] = 1\n",
      "    return rv\n",
      "\n",
      "def train_logistic_regression(labels, features):\n",
      "    \"\"\"Find a weight vector for logistic regression using the supplied\n",
      "    labels and features.\"\"\"\n",
      "    np_labels = np.array(labels)\n",
      "    \n",
      "    initial_w = 0.1 * np.random.random(features.shape[1])\n",
      "    assert initial_w.ndim == 1\n",
      "    \n",
      "    result = opt.fmin_bfgs(\n",
      "        cross_entropy_error, \n",
      "        initial_w, \n",
      "        fprime=logistic_gradient,\n",
      "        args=(features, np_labels),\n",
      "        maxiter=100\n",
      "    )\n",
      "    \n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(cross_entropy_error(np.random.random(data.shape[1]), data, ys))\n",
      "print(logistic_gradient(np.random.random(data.shape[1]), data, ys))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Call to cross_entropy_error took 0.11084891499922378s\n",
        "22.2038705465\n",
        "Call to logistic_gradient took 0.1419852869985334s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 0.0038      0.0012      0.0026     ...,  0.0186      0.0026      0.49657199]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ws = train_logistic_regression(ys, data)\n",
      "predictions = logistic_predict(ws, data)\n",
      "np.sum(predictions != ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Call to logistic_gradient took 0.1728271619977022s\n",
        "Call to cross_entropy_error took 0.06794824700045865s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Call to cross_entropy_error took 0.060120279998955084s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Call to logistic_gradient took 0.1302076850006415s"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}