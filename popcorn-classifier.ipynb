{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for Kaggle popcorn competition\n",
    "\n",
    "Uses logistic regression to try and predict sentiment from a bag-of-words summary of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "# Expit is just the sigmoid function, written in C\n",
    "from scipy.special import expit\n",
    "\n",
    "data = pd.read_csv('even_better_features.csv')\n",
    "\n",
    "ys = data.iloc[:, 0]\n",
    "data = data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['ones_column'] = np.ones_like(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from timeit import default_timer\n",
    "\n",
    "def timed(f):\n",
    "    \"\"\"Decorator to measure the runtime of a function and print it to\n",
    "    stdout. May not be required.\"\"\"\n",
    "    if hasattr(f, 'func_name'):\n",
    "        name = f.func_name\n",
    "    elif hasattr(f, 'im_func'):\n",
    "        name = f.im_func.func_name\n",
    "    elif hasattr(f, '__name__'):\n",
    "        name = f.__name__\n",
    "    else:\n",
    "        name = '<unknown>'\n",
    "\n",
    "    @wraps(f)\n",
    "    def inner(*args, **kwargs):\n",
    "        print(\"Calling {}\".format(name))\n",
    "        start = default_timer()\n",
    "        rv = f(*args, **kwargs)\n",
    "        taken = default_timer() - start\n",
    "        print(\"Call to {} took {}s\".format(name, taken))\n",
    "        return rv\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_sigmoid(x):\n",
    "    \"\"\"Computes sigmoid function, but in the numerically stable way used by Sklearn\"\"\"\n",
    "    gt_zero = x > 0\n",
    "    gt_zero_x = x[gt_zero]\n",
    "    not_gt_zero_x = x[~gt_zero]\n",
    "    rv = np.zeros_like(x)\n",
    "    rv[gt_zero] = 1.0 / (1 + np.exp(-gt_zero_x))\n",
    "    rv[~gt_zero] = np.exp(not_gt_zero_x) / (np.exp(not_gt_zero_x) + 1)\n",
    "    return rv\n",
    "\n",
    "def safe_log_sigmoid(x):\n",
    "    \"\"\"Compute log(sigmoid(x)) safely\"\"\"\n",
    "    rv = np.zeros(len(x))\n",
    "    mask = x > 0\n",
    "    rv[mask] = -np.log(1 + np.exp(-x[mask]))\n",
    "    rv[~mask] = x[~mask] - np.log(1 + np.exp(x[~mask]))\n",
    "    return rv\n",
    "\n",
    "def safe_log_one_minus_sigmoid(x):\n",
    "    \"\"\"Compute log(1 - sigmoid(x)) safely\"\"\"\n",
    "    rv = np.zeros(len(x))\n",
    "    mask = x > 0\n",
    "    rv[mask] = -x[mask] - np.log(1 + np.exp(-x[mask]))\n",
    "    rv[~mask] = -np.log(np.exp(x[~mask]) + 1)\n",
    "    return rv\n",
    "\n",
    "def logistic_gradient(weights, features, labels):\n",
    "    \"\"\"Compute the gradient of the cross-entropy error for logistic regression\"\"\"\n",
    "    predictions = expit(np.dot(features, weights))\n",
    "    error = predictions - labels\n",
    "    # We divide by label.size to try and keep numerical error under control\n",
    "    rv = np.dot(error, features) / labels.size\n",
    "    return rv\n",
    "\n",
    "def cross_entropy_error(weights, features, labels):\n",
    "    \"\"\"Cross-entropy error for logistic regression\"\"\"\n",
    "    dots = np.dot(features, weights)\n",
    "    vect = labels*safe_log_sigmoid(dots) + (1-labels)*safe_log_one_minus_sigmoid(dots)\n",
    "    rv = -np.mean(vect)\n",
    "    return rv\n",
    "\n",
    "def logistic_predict(weights, features):\n",
    "    \"\"\"Predict labels for the given features using the given weights\"\"\"\n",
    "    probs = expit(np.dot(features, weights))\n",
    "    rv = np.zeros(len(features))\n",
    "    rv[probs > 0.5] = 1\n",
    "    return rv\n",
    "\n",
    "def train_logistic_regression(labels, features):\n",
    "    \"\"\"Find a weight vector for logistic regression using the supplied\n",
    "    labels and features.\"\"\"\n",
    "    np_labels = np.array(labels)\n",
    "    \n",
    "    initial_w = 0.1 * np.random.random(features.shape[1])\n",
    "    assert initial_w.ndim == 1\n",
    "    \n",
    "    result = opt.fmin_cg(\n",
    "        cross_entropy_error, \n",
    "        initial_w, \n",
    "        fprime=logistic_gradient,\n",
    "        args=(features, np_labels),\n",
    "        maxiter=20\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.2889745619\n",
      "[ 0.0038      0.0012      0.0026     ...,  0.0186      0.0026      0.49656414]\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy_error(np.random.random(data.shape[1]), data, ys))\n",
    "print(logistic_gradient(np.random.random(data.shape[1]), data, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ws = train_logistic_regression(ys, data)\n",
    "# predictions = logistic_predict(ws, data)\n",
    "# MAXIMUM OVERFIT\n",
    "# np.sum(predictions != ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier using scikit-learn\n",
    "\n",
    "The following classifier uses scikit-learn's SVM implementation to predict whether the movie gets a thumbs up or thumbs down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# From example\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(data[:2500], ys[:2500])\n",
    "# predictions = clf.predict(data[2500:])\n",
    "# print(\"Correct/total: {}/{}\".format(\n",
    "#     np.sum(predictions == ys[2500:]), len(ys[:2500])\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AdaBoost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1500/2500\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostLearner(object):\n",
    "    def fit(self, weights, labels, features):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, features):\n",
    "        pass\n",
    "    \n",
    "class DecisionStump(AdaBoostLearner):\n",
    "    def __init__(self, axis=None):\n",
    "        self.axis = axis\n",
    "        # Is the class 1 mean greater than the class 0 mean?\n",
    "        self.greater = False\n",
    "        self.threshold = None\n",
    "\n",
    "    def fit(self, weights, labels, features):\n",
    "        if self.axis is None:\n",
    "            self.axis = np.random.randint(len(features))\n",
    "        ones_mask = labels == 1\n",
    "        ones_feats = features[ones_mask,self.axis]\n",
    "        ones_mean = np.mean(ones_feats)\n",
    "        zeros_feats = features[~ones_mask,self.axis]\n",
    "        zeros_mean = np.mean(zeros_feats)\n",
    "        self.greater = ones_mean >= zeros_mean\n",
    "        self.threshold = np.mean([ones_mean, zeros_mean])\n",
    "    \n",
    "    def predict(self, features):\n",
    "        assert self.axis is not None, \"Train to get axis\"\n",
    "        axis_values = features[:,self.axis].flatten()\n",
    "        rv = np.zeros(len(features))\n",
    "        mask = axis_values >= self.threshold if self.greater else axis_values < self.threshold\n",
    "        rv[mask] = 1\n",
    "        return rv\n",
    "    \n",
    "class SVMLearner(AdaBoostLearner):\n",
    "    def fit(self, weights, labels, features):\n",
    "        self.clf = svm.SVC()\n",
    "        normed = weights / np.sum(weights)\n",
    "        indices = np.random.choice(\n",
    "            np.arange(len(weights)),\n",
    "            size=len(weights),\n",
    "            replace=True,\n",
    "            p=normed\n",
    "        )\n",
    "        self.clf.fit(features[indices], labels[indices])\n",
    "    \n",
    "    def predict(self, features):\n",
    "        rv = self.clf.predict(features)\n",
    "        return rv\n",
    "\n",
    "def train_adaboost(labels, features, learners=tuple()):\n",
    "    weights = np.ones_like(labels) / len(features)\n",
    "    alphas = np.zeros(len(learners))\n",
    "    for m in range(len(learners)):\n",
    "        learners[m].fit(weights, labels, features)\n",
    "        preds = learners[m].predict(features)\n",
    "        mp = preds != labels\n",
    "        epsilon = np.sum(weights[mp]) / np.sum(weights)\n",
    "        if epsilon == 0:\n",
    "            # OMG WE HAVE A PERFECT CLASSIFIER\n",
    "            break\n",
    "        alphas[m] = np.log((1 - epsilon) / epsilon)\n",
    "        weights[mp] *= np.exp(alphas[m])\n",
    "    return alphas\n",
    "\n",
    "def adaboost_predict(alphas, learners, features):\n",
    "    preds = np.zeros((len(alphas), len(features)))\n",
    "    for m in range(len(learners)):\n",
    "        preds[m] = learners[m].predict(features)\n",
    "    lc = np.dot(alphas, preds)\n",
    "    rv = np.zeros(len(features))\n",
    "    rv[lc > 0] = 1\n",
    "    return rv\n",
    "\n",
    "real_data = data.as_matrix()\n",
    "real_ys = ys.as_matrix()\n",
    "train_set = (real_ys[:2500], real_data[:2500])\n",
    "test_set = (real_ys[2500:], real_data[2500:])\n",
    "learners = [SVMLearner() for m in range(5)]\n",
    "alphas = train_adaboost(train_set[0], train_set[1], learners)\n",
    "preds = adaboost_predict(alphas, learners, test_set[1])\n",
    "correct = np.sum(preds == test_set[0])\n",
    "print(\"Correct: {}/{}\".format(correct, len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
